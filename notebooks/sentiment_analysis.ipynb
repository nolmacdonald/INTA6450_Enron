{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTHOR:** David Mara\n",
    "\n",
    "**DATE OF LAST SIGNIFICANT UPDATE:** 2024-NOV-24\n",
    "\n",
    "**DESCRIPTION:** Enron Corpus Natural Language Processing (NLP) topic modeling\n",
    "\n",
    "**GITHUB ISSUE #1:** https://github.com/nolmacdonald/INTA6450_Enron/issues/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to identify and filter out emails with sentiment scores that fall below a predefined \"neutral\" threshold. This allows for prioritizing emotionally charged or potentially problematic emails, such as those with negative sentiment. [SentimentAnalyzer](https://www.nltk.org/howto/sentiment.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macOS: Using `nltk.download('all')` in a Jupyter notebook will download the data \n",
    "to the wrong location at `/Users/username/nltk_data`.\n",
    "\n",
    "To fix this, run the following command in a terminal:\n",
    "    \n",
    "```shell\n",
    "$ sudo python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "```\n",
    "\n",
    "This will save `nltk_data` for all modules in `/usr/local/share/`.\n",
    "If you only need a certain subset of NLTK data, then you can change the last command `all`,\n",
    "which defines download all NLTK data.\n",
    "For example, you can use the tokenizer, `punkt`, or for sentiment analysis you can use `vader_lexicon`.\n",
    "\n",
    "The optimal way is to save the path that NLTK looks for the data by default in your shell configuration.\n",
    "After adding `NLTK_DATA` to your shell configuration, restart the shell and download the data with\n",
    "the dynamic linking to the path.\n",
    "\n",
    "```shell\n",
    "# NLTK Data Path in ~/.bashrc or ~/.zshrc\n",
    "export NLTK_DATA=\"/usr/local/share/nltk_data\"\n",
    "# Restart the shell\n",
    "$ source ~/.zshrc\n",
    "# Download all NLTK data\n",
    "$ sudo python -m nltk.downloader -d $NLTK_DATA all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# Run once here or in terminal\n",
    "# nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>subject</th>\n",
       "      <th>cc</th>\n",
       "      <th>bcc</th>\n",
       "      <th>mime-version</th>\n",
       "      <th>content-type</th>\n",
       "      <th>content-transfer-encoding</th>\n",
       "      <th>x-from</th>\n",
       "      <th>x-to</th>\n",
       "      <th>x-cc</th>\n",
       "      <th>x-bcc</th>\n",
       "      <th>folder</th>\n",
       "      <th>origin</th>\n",
       "      <th>filename</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks so much.  \\n\\n -----Original Message---...</td>\n",
       "      <td>&lt;19486923.1075862012747.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Thu, 8 Nov 2001 11:24:50 -0800 (PST)</td>\n",
       "      <td>matt.smith@enron.com</td>\n",
       "      <td>kam.keiser@enron.com</td>\n",
       "      <td>RE: new books</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td></td>\n",
       "      <td>Smith, Matt &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=M...</td>\n",
       "      <td>Keiser, Kam &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=K...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\MSMITH18 (Non-Privileged)\\Smith, Matt\\Sent Items</td>\n",
       "      <td>Smith-M</td>\n",
       "      <td>MSMITH18 (Non-Privileged).pst</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carol St. Clair\\nEB 3892\\n713-853-3989 (Phone)...</td>\n",
       "      <td>&lt;12570643.1075842116980.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Tue, 9 May 2000 09:37:00 -0700 (PDT)</td>\n",
       "      <td>carol.clair@enron.com</td>\n",
       "      <td>russell.diamond@enron.com</td>\n",
       "      <td>American Central</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td></td>\n",
       "      <td>Carol St Clair</td>\n",
       "      <td>Russell Diamond</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\Carol_StClair_Dec2000_1\\Notes Folders\\Sent</td>\n",
       "      <td>STCLAIR-C</td>\n",
       "      <td>cstclai.nsf</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nPlease see attached.  Hard copies are being ...</td>\n",
       "      <td>&lt;21222840.1075861608475.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Tue, 20 Nov 2001 10:39:17 -0800 (PST)</td>\n",
       "      <td>jfagan@hewm.com</td>\n",
       "      <td>el00-95@listserv.gsa.gov</td>\n",
       "      <td>First Set of Discovery Requests of the Califor...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td></td>\n",
       "      <td>\"Fagan, Joseph H.\" &lt;JFagan@HEWM.COM&gt;</td>\n",
       "      <td>EL00-95@LISTSERV.GSA.GOV</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\JSTEFFE (Non-Privileged)\\Steffes, James D.\\De...</td>\n",
       "      <td>Steffes-J</td>\n",
       "      <td>JSTEFFE (Non-Privileged).pst</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When: Tuesday, April 17, 2001 2:00 PM-4:00 PM ...</td>\n",
       "      <td>&lt;11978852.1075840779217.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Tue, 8 May 2001 12:14:42 -0700 (PDT)</td>\n",
       "      <td>rumaldo.lopez@enron.com</td>\n",
       "      <td>teresa.seibel@enron.com, vasant.shanbhogue@enr...</td>\n",
       "      <td>Moody's/Famas Credit Scoring Models--Moody's j...</td>\n",
       "      <td>william.bradford@enron.com, vince.kaminski@enr...</td>\n",
       "      <td>william.bradford@enron.com, vince.kaminski@enr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td></td>\n",
       "      <td>Lopez, Rumaldo &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/C...</td>\n",
       "      <td>Teresa Seibel, Vasant Shanbhogue, Rabi De, Rud...</td>\n",
       "      <td>William S Bradford, Vince J Kaminski</td>\n",
       "      <td></td>\n",
       "      <td>\\vkamins\\Calendar</td>\n",
       "      <td>KAMINSKI-V</td>\n",
       "      <td>vincent kaminski 1-30-02.pst</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'll get you my comments ASAP.  Has Shelley Co...</td>\n",
       "      <td>&lt;12733008.1075843217120.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Wed, 13 Dec 2000 02:27:00 -0800 (PST)</td>\n",
       "      <td>jeff.dasovich@enron.com</td>\n",
       "      <td>sarah.novosel@enron.com</td>\n",
       "      <td>Re: Enron Response to San Diego Request for Ga...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td></td>\n",
       "      <td>Jeff Dasovich</td>\n",
       "      <td>Sarah Novosel</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\Jeff_Dasovich_Dec2000\\Notes Folders\\Sent</td>\n",
       "      <td>DASOVICH-J</td>\n",
       "      <td>jdasovic.nsf</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Thanks so much.  \\n\\n -----Original Message---...   \n",
       "1  Carol St. Clair\\nEB 3892\\n713-853-3989 (Phone)...   \n",
       "2  \\nPlease see attached.  Hard copies are being ...   \n",
       "3  When: Tuesday, April 17, 2001 2:00 PM-4:00 PM ...   \n",
       "4  I'll get you my comments ASAP.  Has Shelley Co...   \n",
       "\n",
       "                                      message_id  \\\n",
       "0  <19486923.1075862012747.JavaMail.evans@thyme>   \n",
       "1  <12570643.1075842116980.JavaMail.evans@thyme>   \n",
       "2  <21222840.1075861608475.JavaMail.evans@thyme>   \n",
       "3  <11978852.1075840779217.JavaMail.evans@thyme>   \n",
       "4  <12733008.1075843217120.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    date                     from  \\\n",
       "0   Thu, 8 Nov 2001 11:24:50 -0800 (PST)     matt.smith@enron.com   \n",
       "1   Tue, 9 May 2000 09:37:00 -0700 (PDT)    carol.clair@enron.com   \n",
       "2  Tue, 20 Nov 2001 10:39:17 -0800 (PST)          jfagan@hewm.com   \n",
       "3   Tue, 8 May 2001 12:14:42 -0700 (PDT)  rumaldo.lopez@enron.com   \n",
       "4  Wed, 13 Dec 2000 02:27:00 -0800 (PST)  jeff.dasovich@enron.com   \n",
       "\n",
       "                                                  to  \\\n",
       "0                               kam.keiser@enron.com   \n",
       "1                          russell.diamond@enron.com   \n",
       "2                           el00-95@listserv.gsa.gov   \n",
       "3  teresa.seibel@enron.com, vasant.shanbhogue@enr...   \n",
       "4                            sarah.novosel@enron.com   \n",
       "\n",
       "                                             subject  \\\n",
       "0                                      RE: new books   \n",
       "1                                   American Central   \n",
       "2  First Set of Discovery Requests of the Califor...   \n",
       "3  Moody's/Famas Credit Scoring Models--Moody's j...   \n",
       "4  Re: Enron Response to San Diego Request for Ga...   \n",
       "\n",
       "                                                  cc  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  william.bradford@enron.com, vince.kaminski@enr...   \n",
       "4                                                      \n",
       "\n",
       "                                                 bcc mime-version  \\\n",
       "0                                                             1.0   \n",
       "1                                                             1.0   \n",
       "2                                                             1.0   \n",
       "3  william.bradford@enron.com, vince.kaminski@enr...          1.0   \n",
       "4                                                             1.0   \n",
       "\n",
       "                   content-type content-transfer-encoding  \\\n",
       "0  text/plain; charset=us-ascii                             \n",
       "1  text/plain; charset=us-ascii                             \n",
       "2  text/plain; charset=us-ascii                             \n",
       "3  text/plain; charset=us-ascii                             \n",
       "4  text/plain; charset=us-ascii                             \n",
       "\n",
       "                                              x-from  \\\n",
       "0  Smith, Matt </O=ENRON/OU=NA/CN=RECIPIENTS/CN=M...   \n",
       "1                                     Carol St Clair   \n",
       "2               \"Fagan, Joseph H.\" <JFagan@HEWM.COM>   \n",
       "3  Lopez, Rumaldo </O=ENRON/OU=NA/CN=RECIPIENTS/C...   \n",
       "4                                      Jeff Dasovich   \n",
       "\n",
       "                                                x-to  \\\n",
       "0  Keiser, Kam </O=ENRON/OU=NA/CN=RECIPIENTS/CN=K...   \n",
       "1                                    Russell Diamond   \n",
       "2                           EL00-95@LISTSERV.GSA.GOV   \n",
       "3  Teresa Seibel, Vasant Shanbhogue, Rabi De, Rud...   \n",
       "4                                      Sarah Novosel   \n",
       "\n",
       "                                   x-cc x-bcc  \\\n",
       "0                                               \n",
       "1                                               \n",
       "2                                               \n",
       "3  William S Bradford, Vince J Kaminski         \n",
       "4                                               \n",
       "\n",
       "                                              folder      origin  \\\n",
       "0  \\MSMITH18 (Non-Privileged)\\Smith, Matt\\Sent Items     Smith-M   \n",
       "1        \\Carol_StClair_Dec2000_1\\Notes Folders\\Sent   STCLAIR-C   \n",
       "2  \\JSTEFFE (Non-Privileged)\\Steffes, James D.\\De...   Steffes-J   \n",
       "3                                  \\vkamins\\Calendar  KAMINSKI-V   \n",
       "4          \\Jeff_Dasovich_Dec2000\\Notes Folders\\Sent  DASOVICH-J   \n",
       "\n",
       "                        filename priority  \n",
       "0  MSMITH18 (Non-Privileged).pst   normal  \n",
       "1                    cstclai.nsf   normal  \n",
       "2   JSTEFFE (Non-Privileged).pst   normal  \n",
       "3   vincent kaminski 1-30-02.pst   normal  \n",
       "4                   jdasovic.nsf   normal  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the database (or create it if it doesn't exist)\n",
    "connection = sqlite3.connect(\"../data/emails.db\")\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Load the dataframe from the SQLite database\n",
    "emails_df = pd.read_sql_query(\"SELECT * FROM emails\", connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "# Show email data\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model lists a lot of insignificant information that pre-processing needs to clean up.\n",
    "For example, Topic 0 results have no use as they are `td font com http br tr size width href align`.\n",
    "\n",
    "- Text Extraction: Clean up insignificant data\n",
    "    - Remove HTML Tags: `BeautifulSoup`\n",
    "    - Remove URLs and email addresses\n",
    "- Normalization: Normalize the text by converting to lowercase and removing special characters.\n",
    "- Tokenization: Tokenization splits the text into individual words (tokens).\n",
    "    - `nltk.tokenize.word_tokenize()`\n",
    "- Removing Stop Words: Filter out common stop words that don’t carry significant meaning for topic modeling.\n",
    "    - `nltk.corpus.stopwords()`\n",
    "- Stemming: Stemming reduces words to their root form, which helps group similar words. \n",
    "    - `nltk.stem.PorterStemmer()`\n",
    "- Final Prep: Join the tokens back into sentences for the CountVectorizer to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_17532\\4248341923.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text() if pd.notnull(text) else \"\"\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_17532\\4248341923.py:3: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text() if pd.notnull(text) else \"\"\n"
     ]
    }
   ],
   "source": [
    "def extract_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text() if pd.notnull(text) else \"\"\n",
    "    # Remove URLs and email addresses\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)  # Remove email addresses\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    return text\n",
    "\n",
    "\n",
    "emails_df[\"processed_text\"] = emails_df[\"text\"].apply(extract_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation using string.punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "emails_df[\"processed_text\"] = emails_df[\"processed_text\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization splits the text into individual words (tokens)\n",
    "emails_df[\"tokens\"] = emails_df[\"processed_text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out common stop words that don’t carry significant meaning for topic modeling.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "emails_df[\"tokens\"] = emails_df[\"tokens\"].apply(\n",
    "    lambda x: [word for word in x if word not in stop_words]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming reduces words to their root form, which helps group similar words.\n",
    "stemmer = PorterStemmer()\n",
    "emails_df[\"tokens\"] = emails_df[\"tokens\"].apply(\n",
    "    lambda x: [stemmer.stem(word) for word in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text,sentiment ):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores[sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(get_sentiment,args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m,))\n\u001b[1;32m----> 2\u001b[0m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43memails_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_sentiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(get_sentiment,args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m,))\n\u001b[0;32m      4\u001b[0m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m emails_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(get_sentiment,args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m,))\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1496\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard.<locals>.curried\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurried\u001b[39m(x):\n\u001b[1;32m-> 1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36mget_sentiment\u001b[1;34m(text, sentiment)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentiment\u001b[39m(text,sentiment ):\n\u001b[1;32m----> 2\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     scores \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mpolarity_scores(text)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores[sentiment]\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:341\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload(lexicon_file)\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_lex_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:351\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.make_lex_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    350\u001b[0m     (word, measure) \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 351\u001b[0m     lex_dict[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(measure)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lex_dict\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "emails_df['pos'] = emails_df['processed_text'].head(1000).apply(get_sentiment,args=(\"pos\",))\n",
    "emails_df['neu'] = emails_df['processed_text'].head(1000).apply(get_sentiment,args=(\"neu\",))\n",
    "emails_df['neg'] = emails_df['processed_text'].head(1000).apply(get_sentiment,args=(\"neg\",))\n",
    "emails_df['compound'] = emails_df['processed_text'].head(1000).apply(get_sentiment,args=(\"compound\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
