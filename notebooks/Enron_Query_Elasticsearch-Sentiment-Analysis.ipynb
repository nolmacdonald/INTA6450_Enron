{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Query Elasticsearch Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macOS: Using `nltk.download('all')` in a Jupyter notebook will download the data \n",
    "to the wrong location at `/Users/username/nltk_data`.\n",
    "\n",
    "To fix this, run the following command in a terminal:\n",
    "    \n",
    "```shell\n",
    "$ sudo python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "```\n",
    "\n",
    "This will save `nltk_data` for all modules in `/usr/local/share/`.\n",
    "If you only need a certain subset of NLTK data, then you can change the last command `all`,\n",
    "which defines download all NLTK data.\n",
    "For example, you can use the tokenizer, `punkt`, or for sentiment analysis you can use `vader_lexicon`.\n",
    "\n",
    "The optimal way is to save the path that NLTK looks for the data by default in your shell configuration.\n",
    "After adding `NLTK_DATA` to your shell configuration, restart the shell and download the data with\n",
    "the dynamic linking to the path.\n",
    "\n",
    "```shell\n",
    "# NLTK Data Path in ~/.bashrc or ~/.zshrc\n",
    "export NLTK_DATA=\"/usr/local/share/nltk_data\"\n",
    "# Restart the shell\n",
    "$ source ~/.zshrc\n",
    "# Download all NLTK data\n",
    "$ sudo python -m nltk.downloader -d $NLTK_DATA all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Run once here or in terminal\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wDCRVtiXGS3k",
    "outputId": "08f9ea61-8a38-4c16-c8d1-52e80691fa1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'yellow open enron lVq0is2BTCmgDk2kFyZHTQ 1 1 251735 129380 1.1gb 1.1gb\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import json\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from dateutil import parser\n",
    "\n",
    "# Host for INTA 6450 Class\n",
    "host = 'http://18.188.56.207:9200/'\n",
    "# Request enron corpus database\n",
    "requests.get(host + '_cat/indices/enron').content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-lcMiQfGS3v"
   },
   "outputs": [],
   "source": [
    "def elasticsearch_results_to_df(results):\n",
    "    '''\n",
    "    A function that will take the results of a requests.get \n",
    "    call to Elasticsearch and return a pandas.DataFrame object \n",
    "    with the results \n",
    "    '''\n",
    "    hits = results.json()['hits']['hits']\n",
    "    data = pandas.DataFrame([i['_source'] for i in hits], index = [i['_id'] for i in hits])\n",
    "    data['date'] = data['date'].apply(parser.parse)\n",
    "    return(data)\n",
    "\n",
    "def print_df_row(row):\n",
    "    '''\n",
    "    A function that will take a row of the data frame and print it out\n",
    "    '''\n",
    "    print('____________________')\n",
    "    print('RE: %s' % row.get('subject',''))\n",
    "    print('At: %s' % row.get('date',''))\n",
    "    print('From: %s' % row.get('sender',''))\n",
    "    print('To: %s' % row.get('recipients',''))\n",
    "    print('CC: %s' % row.get('cc',''))\n",
    "    print('BCC: %s' % row.get('bcc',''))\n",
    "    print('Body:\\n%s' % row.get('text',''))\n",
    "    print('____________________')\n",
    "\n",
    "# create preprocess_text function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "def get_sentiment(text,sentiment ):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores[sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "2teqoyC2GS34",
    "outputId": "3205a4cb-cb8b-4af3-dc6c-23ff142e8461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found {'value': 77, 'relation': 'eq'} messages matching the query, of \n"
     ]
    }
   ],
   "source": [
    "# Query For a full text match in the \"text\" field \n",
    "# Uses the \"match\" query: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html\n",
    "doc = {\n",
    "    \"query\": {\n",
    "        \"match_phrase\" : {\n",
    "            \"text\" : \"Account numbers\"\n",
    "        } \n",
    "    },\n",
    "    \"from\" : 0, # Starting message to return. \n",
    "    \"size\" : 2000, # Return this many messages. Can't be more than 10,000\n",
    "}\n",
    "r=requests.get(host + 'enron/_search',\n",
    "               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n",
    "r.raise_for_status()\n",
    "print(\"Found %s messages matching the query, of \" % r.json()['hits']['total'])\n",
    "df = elasticsearch_results_to_df(r)\n",
    "df['processedText'] = df['text'].apply(preprocess_text)\n",
    "df['pos'] = df['processedText'].apply(get_sentiment,args=(\"pos\",))\n",
    "df['neu'] = df['processedText'].apply(get_sentiment,args=(\"neu\",))\n",
    "df['neg'] = df['processedText'].apply(get_sentiment,args=(\"neg\",))\n",
    "df['compound'] = df['processedText'].apply(get_sentiment,args=(\"compound\",))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enron Query Elasticsearch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "enron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
